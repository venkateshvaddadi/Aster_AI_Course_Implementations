{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "M_OLSkbSBOPD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/venkatesh/anaconda3/envs/venkatesh_pytorch_updated_110/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name='mobilenetv2'\n",
    "\n",
    "\n",
    "if(model_name=='mobilenetv2'):\n",
    "    from models.mobilenet import mobilenetv2\n",
    "    model=mobilenetv2(num_classes=3)\n",
    "if(model_name=='ResNet50'):\n",
    "    from models.resnet import ResNet50\n",
    "    model=ResNet50(num_classes=3, channels=3)    \n",
    "if(model_name=='ShuffleNet2'):\n",
    "    from models.ShuffleNet2 import ShuffleNet2\n",
    "    model=ShuffleNet2(num_classes=3)    \n",
    "if(model_name=='SqueezeNet'):\n",
    "    from models.SqueezeNet import SqueezeNet\n",
    "    model=SqueezeNet(num_classes=3).float().cuda()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MobileNetV2(\n",
      "  (features): Sequential(\n",
      "    (0): Sequential(\n",
      "      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (2): ReLU6(inplace=True)\n",
      "    )\n",
      "    (1): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n",
      "        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n",
      "        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n",
      "        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n",
      "        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (8): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (9): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (10): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (11): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
      "        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(384, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (12): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (13): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (14): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(576, 576, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=576, bias=False)\n",
      "        (4): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(576, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (15): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (16): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(160, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (17): InvertedResidual(\n",
      "      (conv): Sequential(\n",
      "        (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU6(inplace=True)\n",
      "        (3): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)\n",
      "        (4): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (5): ReLU6(inplace=True)\n",
      "        (6): Conv2d(960, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (7): BatchNorm2d(320, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU6(inplace=True)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (classifier): Linear(in_features=1280, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making directory for saving models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************\n",
      "saved_models//Jul_29_11_59_am__model_mobilenetv2/\n",
      "Model will be saved to  : saved_models//Jul_29_11_59_am__model_mobilenetv2/\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import os\n",
    "print ('*******************************************************')\n",
    "start_time=time.time()\n",
    "experiments_folder=\"saved_models/\"\n",
    "experiment_name=datetime.now().strftime(\"%b_%d_%I_%M_%P_\")+\"_model_\"+model_name\n",
    "\n",
    "directory=experiments_folder+\"/\"+experiment_name+\"/\"\n",
    "print(directory)\n",
    "print('Model will be saved to  :', directory)\n",
    "\n",
    "try:\n",
    "    os.makedirs(directory)\n",
    "except:\n",
    "    print('model already existed..........')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from models.resnet import ResNet50\n",
    "#model=ResNet50(num_classes=3, channels=3)\n",
    "#model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1722151692928,
     "user": {
      "displayName": "venkatesh vaddadi",
      "userId": "14864650657704592599"
     },
     "user_tz": -330
    },
    "id": "J1Z_F9MLCoHH",
    "outputId": "a5353d3e-f5c3-4917-e1fc-424a74b1e4ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "traing data\n",
      "   Unnamed: 0                                         image_path  \\\n",
      "0        5758  ./processed_data/pocus_videos_to_images/convex...   \n",
      "1        4244  ./processed_data/pocus_videos_to_images/convex...   \n",
      "2        9517  ./processed_data/pocus_videos_to_images/convex...   \n",
      "3         415  ./processed_data/pocus_videos_to_images/linear...   \n",
      "4       12024  ./processed_data/pocus_videos_to_images/convex...   \n",
      "\n",
      "                 label  \n",
      "0                COVID  \n",
      "1              healthy  \n",
      "2                COVID  \n",
      "3              healthy  \n",
      "4  bacterial pneumonia  \n",
      "stastics of the given data:\n",
      "COVID                  3919\n",
      "healthy                3697\n",
      "bacterial pneumonia    2733\n",
      "viral pneumonia         131\n",
      "Name: label, dtype: int64\n",
      "####################################################################################################\n",
      "validation data\n",
      "   Unnamed: 0                                         image_path  \\\n",
      "0       11813  ./processed_data/pocus_videos_to_images/convex...   \n",
      "1        9247  ./processed_data/pocus_videos_to_images/convex...   \n",
      "2       10128  ./processed_data/pocus_videos_to_images/convex...   \n",
      "3        2310  ./processed_data/pocus_videos_to_images/convex...   \n",
      "4        6786  ./processed_data/pocus_videos_to_images/convex...   \n",
      "\n",
      "                 label  \n",
      "0  bacterial pneumonia  \n",
      "1                COVID  \n",
      "2                COVID  \n",
      "3              healthy  \n",
      "4                COVID  \n",
      "stastics of the given data:\n",
      "COVID                  509\n",
      "healthy                439\n",
      "bacterial pneumonia    352\n",
      "viral pneumonia         10\n",
      "Name: label, dtype: int64\n",
      "####################################################################################################\n",
      "test data\n",
      "   Unnamed: 0                                         image_path  \\\n",
      "0        8570  ./processed_data/pocus_videos_to_images/convex...   \n",
      "1       12592  ./processed_data/pocus_videos_to_images/convex...   \n",
      "2         424  ./processed_data/pocus_videos_to_images/linear...   \n",
      "3        8086  ./processed_data/pocus_videos_to_images/convex...   \n",
      "4        6800  ./processed_data/pocus_videos_to_images/convex...   \n",
      "\n",
      "                 label  \n",
      "0                COVID  \n",
      "1  bacterial pneumonia  \n",
      "2              healthy  \n",
      "3                COVID  \n",
      "4                COVID  \n",
      "stastics of the given data:\n",
      "healthy                503\n",
      "COVID                  456\n",
      "bacterial pneumonia    333\n",
      "viral pneumonia         18\n",
      "Name: label, dtype: int64\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print('traing data')\n",
    "data = pd.read_csv('./csv_files/data_train.csv')\n",
    "\n",
    "print(data.head())\n",
    "# Print the statastis of the given data\n",
    "print(\"stastics of the given data:\")\n",
    "print(data['label'].value_counts())\n",
    "print('#'*100)\n",
    "\n",
    "print('validation data')\n",
    "\n",
    "data = pd.read_csv('./csv_files/data_valid.csv')\n",
    "\n",
    "print(data.head())\n",
    "# Print the statastis of the given data\n",
    "print(\"stastics of the given data:\")\n",
    "print(data['label'].value_counts())\n",
    "print('#'*100)\n",
    "\n",
    "print('test data')\n",
    "\n",
    "data = pd.read_csv('./csv_files/data_test.csv')\n",
    "\n",
    "print(data.head())\n",
    "# Print the statastis of the given data\n",
    "print(\"stastics of the given data:\")\n",
    "print(data['label'].value_counts())\n",
    "print('#'*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L0nLytpjCgZF"
   },
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ISZwsT9MRzwg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Custom Dataset class for classification\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file, usecols=['image_path', 'label'])\n",
    "        self.transform = transform\n",
    "        self.class_to_int = {\"COVID\": 0, \"healthy\": 1, \"bacterial pneumonia\": 2,\"viral pneumonia\": 2}  # Mapping class names to integers\n",
    "\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        image_path = self.data['image_path'][idx]\n",
    "        label = self.class_to_int[self.data['label'][idx]]  # Convert class label to integer\n",
    "\n",
    "        # Load image using PIL\n",
    "        image = Image.open(image_path)\n",
    "\n",
    "        # Convert PIL Image to NumPy array\n",
    "        #image = np.array(image)\n",
    "        #image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        #print(image.shape,label)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        image=image[:3,:,:]\n",
    "\n",
    "\n",
    "\n",
    "        #print(np.unique(image))\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "xPo0zQB-Pcnf"
   },
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define transformations for data augmentation or normalization\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize image to required size\n",
    "    transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6862,
     "status": "ok",
     "timestamp": 1722151773370,
     "user": {
      "displayName": "venkatesh vaddadi",
      "userId": "14864650657704592599"
     },
     "user_tz": -330
    },
    "id": "OQgQBIa0R2zJ",
    "outputId": "6fa38740-b6cc-415d-988a-8acbb355b1f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/venkatesh/anaconda3/envs/venkatesh_pytorch_updated_110/lib/python3.9/site-packages (4.64.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1722151773370,
     "user": {
      "displayName": "venkatesh vaddadi",
      "userId": "14864650657704592599"
     },
     "user_tz": -330
    },
    "id": "eRNeYZqWR8cH",
    "outputId": "eefc4d8d-fb7a-4729-9921-e03df941cb40"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "164\n",
      "1310\n",
      "164\n",
      "164\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Assuming you have a CSV file named 'data.csv' with image paths and class labels\n",
    "train_csv_file = './drive/MyDrive/Aster/Aster_AI_Course/TA_Projects_Session_2_LUNG_USG_Classification/csv_files/data_train.csv'\n",
    "\n",
    "# Create CustomDataset instance\n",
    "dataset = CustomDataset('./csv_files/data_train.csv', transform=transform)\n",
    "train_loader = DataLoader(dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "val_dataset = CustomDataset('./csv_files/data_valid.csv', transform=transform)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "test_dataset = CustomDataset('./csv_files/data_test.csv', transform=transform)\n",
    "test_dataloader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(len(test_dataloader))\n",
    "print(len(train_loader))\n",
    "print(len(val_dataloader))\n",
    "print(len(test_dataloader))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "O9K9sfF14Fj9"
   },
   "outputs": [],
   "source": [
    "num_classes = 3  # Assuming 3 classes: \"covid\", \"normal\", \"pneumonia\"\n",
    "\n",
    "\n",
    "learning_rate=0.0001\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "executionInfo": {
     "elapsed": 1402734,
     "status": "error",
     "timestamp": 1722154653137,
     "user": {
      "displayName": "venkatesh vaddadi",
      "userId": "14864650657704592599"
     },
     "user_tz": -330
    },
    "id": "MDAOGA2XGaYV",
    "outputId": "90502478-d614-4152-b892-ae0e84166344"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 1310/1310 [01:28<00:00, 14.82it/s, Loss=0.376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 0 Train mean loss: 492.27969976\n",
      "Train Accuracy%:  84.74236641221374 == 8881 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_0.pth\n",
      "Epoch [1/25], Loss: 0.3758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/25: 100%|██████████| 164/164 [00:08<00:00, 20.18it/s, Validation Loss=0.0945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 0.0945\n",
      "-------------------------------------------------\n",
      "Epoch: 0 Val mean loss: 15.49989851\n",
      "valiation Accuracy%:  97.32824427480917 == 1275 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 1310/1310 [01:25<00:00, 15.29it/s, Loss=0.113]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 1 Train mean loss: 147.87784874\n",
      "Train Accuracy%:  95.99236641221374 == 10060 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_1.pth\n",
      "Epoch [2/25], Loss: 0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/25: 100%|██████████| 164/164 [00:08<00:00, 19.95it/s, Validation Loss=0.0411]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Loss: 0.0411\n",
      "-------------------------------------------------\n",
      "Epoch: 1 Val mean loss: 6.74803541\n",
      "valiation Accuracy%:  98.93129770992367 == 1296 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 1310/1310 [01:25<00:00, 15.29it/s, Loss=0.0333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 2 Train mean loss: 43.58142330\n",
      "Train Accuracy%:  99.01717557251908 == 10377 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_2.pth\n",
      "Epoch [3/25], Loss: 0.0333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/25: 100%|██████████| 164/164 [00:08<00:00, 20.22it/s, Validation Loss=0.0416]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Loss: 0.0416\n",
      "-------------------------------------------------\n",
      "Epoch: 2 Val mean loss: 6.82818585\n",
      "valiation Accuracy%:  98.85496183206106 == 1295 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 1310/1310 [01:25<00:00, 15.38it/s, Loss=0.0216]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 3 Train mean loss: 28.34899383\n",
      "Train Accuracy%:  99.26526717557252 == 10403 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_3.pth\n",
      "Epoch [4/25], Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/25: 100%|██████████| 164/164 [00:07<00:00, 20.56it/s, Validation Loss=0.0342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Loss: 0.0342\n",
      "-------------------------------------------------\n",
      "Epoch: 3 Val mean loss: 5.60967046\n",
      "valiation Accuracy%:  98.93129770992367 == 1296 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.12it/s, Loss=0.0279]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 4 Train mean loss: 36.60251906\n",
      "Train Accuracy%:  98.97900763358778 == 10373 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_4.pth\n",
      "Epoch [5/25], Loss: 0.0279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/25: 100%|██████████| 164/164 [00:08<00:00, 20.25it/s, Validation Loss=0.0106] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Loss: 0.0106\n",
      "-------------------------------------------------\n",
      "Epoch: 4 Val mean loss: 1.74270600\n",
      "valiation Accuracy%:  99.77099236641222 == 1307 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.13it/s, Loss=0.0176] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 5 Train mean loss: 23.02955899\n",
      "Train Accuracy%:  99.4942748091603 == 10427 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_5.pth\n",
      "Epoch [6/25], Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/25: 100%|██████████| 164/164 [00:08<00:00, 20.27it/s, Validation Loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Loss: 0.0256\n",
      "-------------------------------------------------\n",
      "Epoch: 5 Val mean loss: 4.20008027\n",
      "valiation Accuracy%:  99.16030534351145 == 1299 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 1310/1310 [01:25<00:00, 15.25it/s, Loss=0.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 6 Train mean loss: 21.20767347\n",
      "Train Accuracy%:  99.44656488549619 == 10422 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_6.pth\n",
      "Epoch [7/25], Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/25: 100%|██████████| 164/164 [00:07<00:00, 20.70it/s, Validation Loss=0.024]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Loss: 0.0240\n",
      "-------------------------------------------------\n",
      "Epoch: 6 Val mean loss: 3.94199465\n",
      "valiation Accuracy%:  99.46564885496183 == 1303 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.10it/s, Loss=0.0224] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 7 Train mean loss: 29.29307407\n",
      "Train Accuracy%:  99.44656488549619 == 10422 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_7.pth\n",
      "Epoch [8/25], Loss: 0.0224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/25: 100%|██████████| 164/164 [00:08<00:00, 20.13it/s, Validation Loss=0.0162]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Loss: 0.0162\n",
      "-------------------------------------------------\n",
      "Epoch: 7 Val mean loss: 2.65164535\n",
      "valiation Accuracy%:  99.8473282442748 == 1308 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.17it/s, Loss=0.0158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 8 Train mean loss: 20.74974204\n",
      "Train Accuracy%:  99.54198473282443 == 10432 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_8.pth\n",
      "Epoch [9/25], Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/25: 100%|██████████| 164/164 [00:08<00:00, 20.48it/s, Validation Loss=0.0107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Loss: 0.0107\n",
      "-------------------------------------------------\n",
      "Epoch: 8 Val mean loss: 1.75438716\n",
      "valiation Accuracy%:  99.8473282442748 == 1308 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.16it/s, Loss=0.00919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 9 Train mean loss: 12.03271723\n",
      "Train Accuracy%:  99.62786259541984 == 10441 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_9.pth\n",
      "Epoch [10/25], Loss: 0.0092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/25: 100%|██████████| 164/164 [00:08<00:00, 19.95it/s, Validation Loss=0.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Loss: 0.0388\n",
      "-------------------------------------------------\n",
      "Epoch: 9 Val mean loss: 6.36808512\n",
      "valiation Accuracy%:  98.62595419847328 == 1292 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.23it/s, Loss=0.0164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 10 Train mean loss: 21.45789768\n",
      "Train Accuracy%:  99.47519083969466 == 10425 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_10.pth\n",
      "Epoch [11/25], Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/25: 100%|██████████| 164/164 [00:08<00:00, 20.32it/s, Validation Loss=0.0174]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Loss: 0.0174\n",
      "-------------------------------------------------\n",
      "Epoch: 10 Val mean loss: 2.85345917\n",
      "valiation Accuracy%:  99.69465648854961 == 1306 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 1310/1310 [01:27<00:00, 15.02it/s, Loss=0.0042] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 11 Train mean loss: 5.50210282\n",
      "Train Accuracy%:  99.8854961832061 == 10468 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_11.pth\n",
      "Epoch [12/25], Loss: 0.0042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/25: 100%|██████████| 164/164 [00:08<00:00, 19.81it/s, Validation Loss=0.0104] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Loss: 0.0104\n",
      "-------------------------------------------------\n",
      "Epoch: 11 Val mean loss: 1.71128882\n",
      "valiation Accuracy%:  99.77099236641222 == 1307 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.11it/s, Loss=0.0108] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 12 Train mean loss: 14.13128344\n",
      "Train Accuracy%:  99.69465648854961 == 10448 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_12.pth\n",
      "Epoch [13/25], Loss: 0.0108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/25: 100%|██████████| 164/164 [00:08<00:00, 20.08it/s, Validation Loss=0.111] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Loss: 0.1109\n",
      "-------------------------------------------------\n",
      "Epoch: 12 Val mean loss: 18.18634365\n",
      "valiation Accuracy%:  97.25190839694656 == 1274 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.07it/s, Loss=0.0107] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 13 Train mean loss: 14.07916375\n",
      "Train Accuracy%:  99.68511450381679 == 10447 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_13.pth\n",
      "Epoch [14/25], Loss: 0.0107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/25: 100%|██████████| 164/164 [00:08<00:00, 20.20it/s, Validation Loss=0.0809]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Loss: 0.0809\n",
      "-------------------------------------------------\n",
      "Epoch: 13 Val mean loss: 13.26789225\n",
      "valiation Accuracy%:  98.09160305343511 == 1285 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 1310/1310 [01:26<00:00, 15.10it/s, Loss=0.00779]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------\n",
      "Epoch: 14 Train mean loss: 10.20162434\n",
      "Train Accuracy%:  99.78053435114504 == 10457 / 10480\n",
      "-------------------------------------------------\n",
      "Model saved at: saved_models//Jul_29_11_59_am__model_mobilenetv2//mymodel_14.pth\n",
      "Epoch [15/25], Loss: 0.0078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/25: 100%|██████████| 164/164 [00:08<00:00, 19.78it/s, Validation Loss=0.0134] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Loss: 0.0134\n",
      "-------------------------------------------------\n",
      "Epoch: 14 Val mean loss: 2.20502418\n",
      "valiation Accuracy%:  99.8473282442748 == 1308 / 1310\n",
      "-------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/25:   7%|▋         | 90/1310 [00:06<01:19, 15.26it/s, Loss=0.00279] "
     ]
    }
   ],
   "source": [
    "# Train your model\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define your training function\n",
    "model.train()\n",
    "train_losses = []  # List to store training losses\n",
    "val_losses = []  # List to store validaion losses\n",
    "\n",
    "num_epochs=25\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    validation_loss=0.0\n",
    "    \n",
    "    y_true_train = []\n",
    "    y_pred_train = []\n",
    "\n",
    "\n",
    "    # Create tqdm progress bar\n",
    "    progress_bar = tqdm(enumerate(train_loader), total=len(train_loader), desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    for batch_idx, (images, labels) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #images=images/255\n",
    "        #print(torch.unique(images))\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        y_pred_train.extend(outputs.detach().argmax(dim=-1).tolist())\n",
    "        y_true_train.extend(labels.detach().tolist())\n",
    "\n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix({'Loss': running_loss / (batch_idx + 1)})\n",
    "\n",
    "        \n",
    "    total_correct = len([True for x, y in zip(y_pred_train, y_true_train) if x==y])\n",
    "    total = len(y_pred_train)\n",
    "    accuracy = total_correct * 100 / total\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Epoch: {} Train mean loss: {:.8f}\".format(epoch, running_loss))\n",
    "    print(\"Train Accuracy%: \", accuracy, \"==\", total_correct, \"/\", total)\n",
    "    print(\"-------------------------------------------------\")\n",
    "        \n",
    "    save_path=directory+'/mymodel_'+str(epoch)+'.pth'\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved at: {save_path}')\n",
    "\n",
    "    # storing the train losses\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    progress_bar = tqdm(enumerate(val_dataloader), total=len(val_dataloader), desc=f'Epoch {epoch + 1}/{num_epochs}')\n",
    "\n",
    "    y_true_val = []\n",
    "    y_pred_val = []\n",
    "\n",
    "    \n",
    "    model.eval()\n",
    "    for batch_idx, (images, labels) in progress_bar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        #images=images/255\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "        y_pred_val.extend(outputs.detach().argmax(dim=-1).tolist())\n",
    "        y_true_val.extend(labels.detach().tolist())\n",
    "\n",
    "        \n",
    "        \n",
    "        # Update progress bar description with current loss\n",
    "        progress_bar.set_postfix({'Validation Loss': validation_loss / (batch_idx + 1)})\n",
    "\n",
    "    # storing the validation losses\n",
    "\n",
    "    epoch_loss = validation_loss / len(val_dataloader)\n",
    "    val_losses.append(epoch_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {epoch_loss:.4f}')\n",
    "    \n",
    "    total_correct = len([True for x, y in zip(y_pred_val, y_true_val) if x==y])\n",
    "    total = len(y_true_val)\n",
    "    accuracy = total_correct * 100 / total\n",
    "\n",
    "    print(\"-------------------------------------------------\")\n",
    "    print(\"Epoch: {} Val mean loss: {:.8f}\".format(epoch, validation_loss))\n",
    "    print(\"valiation Accuracy%: \", accuracy, \"==\", total_correct, \"/\", total)\n",
    "    print(\"-------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GMcEDHbSNj0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training and validation losses\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i5Dttl9Poa-C"
   },
   "source": [
    "# Model Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Total params: 2,227,715\n",
    "\n",
    "Trainable params: 2,227,715\n",
    "\n",
    "Non-trainable params: 0\n",
    "----------------------------------------------------------------\n",
    "Input size (MB): 0.57\n",
    "\n",
    "Forward/backward pass size (MB): 152.86\n",
    "\n",
    "Params size (MB): 8.50\n",
    "\n",
    "Estimated Total Size (MB): 161.93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computational complexity: 911.67 MMac\n",
    "\n",
    "\n",
    "Computational complexity: 1823.34 MFlops\n",
    "\n",
    "Number of parameters: 2.23 M "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3620,
     "status": "ok",
     "timestamp": 1722149595965,
     "user": {
      "displayName": "venkatesh vaddadi",
      "userId": "14864650657704592599"
     },
     "user_tz": -330
    },
    "id": "hh81HlXpayBj",
    "outputId": "38772ac0-a5f4-47b2-9df7-1a9db90fb2b7"
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from ptflops import get_model_complexity_info\n",
    "from pthflops import count_ops\n",
    "\n",
    "macs, params = get_model_complexity_info(model, (3, 448, 320), as_strings=True,print_per_layer_stat=False, verbose=False)\n",
    "# # Extract the numerical value\n",
    "flops = eval(re.findall(r'([\\d.]+)', macs)[0])*2\n",
    "# Extract the unit\n",
    "flops_unit = re.findall(r'([A-Za-z]+)', macs)[0][0]\n",
    "\n",
    "print('Computational complexity: {:<8}'.format(macs))\n",
    "print('Computational complexity: {} {}Flops'.format(flops, flops_unit))\n",
    "print('Number of parameters: {:<8}'.format(params))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choosing the best model from validation loss.\n",
    "\n",
    "best_epoch=np.argmin(val_losses)\n",
    "print('best_epoch:',best_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 396,
     "status": "ok",
     "timestamp": 1722092520501,
     "user": {
      "displayName": "venkatesh vaddadi",
      "userId": "14864650657704592599"
     },
     "user_tz": -330
    },
    "id": "w_sacd3gtlMe",
    "outputId": "ab2e3264-a5d3-4410-82ce-29109f63fa8d"
   },
   "outputs": [],
   "source": [
    "ckpt = torch.load('./saved_models/mymodel_'+str(best_epoch)+'.pth')\n",
    "model.load_state_dict(ckpt)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "progress_bar = tqdm(enumerate(test_dataloader), total=len(test_dataloader), desc=f'testing')\n",
    "\n",
    "y_true_val = []\n",
    "y_pred_val = []\n",
    "\n",
    "\n",
    "model.eval()\n",
    "for batch_idx, (images, labels) in progress_bar:\n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "    #images=images/255\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs = model(images)\n",
    "\n",
    "    y_pred_val.extend(outputs.detach().argmax(dim=-1).tolist())\n",
    "    y_true_val.extend(labels.detach().tolist())\n",
    "\n",
    "\n",
    "\n",
    "    # Update progress bar description with current loss\n",
    "\n",
    "\n",
    "\n",
    "total_correct = len([True for x, y in zip(y_pred_val, y_true_val) if x==y])\n",
    "total = len(y_true_val)\n",
    "accuracy = total_correct * 100 / total\n",
    "\n",
    "print(\"-------------------------------------------------\")\n",
    "print(\"TEST Accuracy (%): \", accuracy, \"==\", total_correct, \"/\", total)\n",
    "print(\"-------------------------------------------------\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
